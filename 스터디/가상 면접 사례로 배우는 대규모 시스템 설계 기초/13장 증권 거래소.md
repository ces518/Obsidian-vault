# 개요

**거래소의 본질:** 거래소의 기본 기능은 **구매자와 판매자를 효율적으로 연결**하는 것

**거래 방식의 진화:**
- 과거엔 사람끼리 직접(물물교환/외침/눈으로 확인 가능한 상품) 거래하던 것을,
- 오늘날엔 **주문 처리와 매칭을 컴퓨터(고성능 시스템)가 전자적으로 자동 처리**.
- 거래 목적도 단순 교환을 넘어 **차익 실현/중개** 등으로 확대.

**거래소의 다양성:** NYSE, 나스닥 같은 전통 강자 외에도
- 특정 금융 세그먼트에 집중하거나,
- 기술을 특히 강조하거나,
- **공정성**에 초점을 둔 형태 등 다양한 거래소가 존재한다

**규모 감(트래픽/볼륨) 예시:**
- **NYSE**: 하루에 “수십 억 건” 규모의 거래(주문/거래량 맥락) 처리 언급
- **HKEX**: 하루에 “약 2천억 건”의 주식 거래 처리 언급
- 그리고 **시가총액 기준 ‘1조 달러 클럽’ 대형 거래소**를 그림(13.1)으로 소개.

**첨언**
- “주문 수”와 “체결 수”는 스케일이 다르고(대부분 주문은 미체결/취소로 끝남), 시스템 설계에서는 **주문 이벤트 처리량**이 병목이 되는 경우가 많음
    
- 거래소를 “웹 서비스”처럼 보면 안 되는 이유가 여기서 이미 드러남: **결정론/공정성/규제/초저지연**이 도메인 핵심값.
---

# 요구사항

### 1. 거래 대상
- **주식만 거래**
    - 옵션, 선물, 파생상품 ❌
    - → 도메인 복잡도 축소
### 2. 주문 유형
- 지원:
    - **신규 주문**
    - **체결되지 않은 주문 취소**
- 주문 타입:
    - ✅ **지정가 주문 (Limit Order)**
    - ❌ 시장가 주문
    - ❌ 조건부 주문

📌 의미
→ 매칭 로직과 호가창(order book)이 **가격 우선 + 시간 우선** 구조로 단순화됨

### 3. 거래 시간
- **정규 장중 거래만**
    - 시간 외 거래(after-hours) ❌
### 4. 핵심 기능
- 새로운 지정가 주문 접수
- 미체결 주문 취소
- **체결 시 실시간 알림**
- **호가창(order book) 실시간 갱신**
    - 매수/매도 주문 목록
### 5. 사용자·규모
- 동시 사용자: **수만 명**
- 종목 수: **최소 100개 주식**
- 주문량: **하루 수십억 건**
→ 소규모 아님, **중~대규모 거래소**

### 6. 규제 요구사항 

- 거래소는 **규제 시설(regulated facility)**
- **Risk Check(위험성 점검)** 필수
    - 예:
        - 사용자 1명이 하루에 AAPL 100만 주 이상 거래 ❌
- **주문 전 규칙 위반 여부를 반드시 검증**

### 7. 자금(지갑) 관리

- 주문 전에:
    - **충분한 자금 보유 여부 확인**
- 미체결 주문에 사용된 자금:
    - **락(lock) 처리**
    - 다른 주문에 재사용 ❌

📌 이 부분이 핵심
- 단순 매칭 시스템 ❌
- **자금 원장 + 주문 원장 + 락 자금 관리**가 함께 가야 함
    
**첨언**
- 현실에서는 여기서 **Pre-trade risk**가 더 세분화됨: 계좌 상태(정지/제재), 종목 제한, 단일 주문 최대 수량/금액, 가격 밴드(상하한/변동성 완화), 포지션 제한, 레버리지/증거금 등.
- “자금 락”은 단순 boolean이 아니라 보통 **가용잔고(available) / 주문잠김(reserved) / 정산대기(pending) / 확정(settled)** 같은 상태 모델로 커짐.

---

# 비기능 요구사항

### 1. 가용성 (Availability)
- **최소 99.99%**
- 몇 초 장애 → 신뢰·평판 치명적

### 2. 결함 내성 (Fault Tolerance)

- 장애 발생 시:
    - **파급 최소화**
    - **빠른 복구 메커니즘 필수**

### 3. 지연 시간 (Latency)

- **밀리초(ms) 단위**
- 특히 **p99 지연 시간** 중요
    - 일부 사용자라도 느리면 거래 경험 망함
- 왕복 지연:
    - 주문 접수 → 체결 결과 반환

### 4. 보안 (Security)
- 계정 관리 시스템 필수
- **KYC (고객 신원 확인)** 필요
- 공개 리소스:
    - **DDoS 방어** 필수 (시장 데이터 페이지 등)
        

## 5. 개략적 트래픽 규모 계산
- 종목 수: 100
- 주문 수: 하루 **10억 건**
- 거래 시간: 하루 **6.5시간**
    

**평균 QPS**
`10억 / (6.5 × 3600) ≈ 43,000 QPS`

**최대 QPS (피크타임)**
`약 5배 → 215,000 QPS`

📌 장 시작 직후 / 장 마감 직전이 피크

- 핵심은:
    - **규모**
    - **규제**
    - **자금 락**
    - **p99 latency**
        
- 면접관은 여기서:
    - “이 사람이 **증권 시스템을 ‘웹 서비스’로 착각하지 않는지**”를 봄
        

**첨언**
- 거래소/브로커 트래픽은 “평균”보다 **장 오픈/클로즈 + 뉴스 이벤트 + 변동성 급등** 같은 **버스트**가 훨씬 중요.
    
- p99을 깨는 대표 원인은 네트워크/GC 말고도 **락 경합, false sharing, context switch, CPU 주파수 변동, NUMA, IRQ** 같은 “하드웨어 레벨” 요인까지 내려감.

---

# 도메인 정리

### 1. 브로커 시스템
- 개인 사용자는 **거래소에 직접 접속하지 않음**
- 예:
    - Charles Schwab, Robinhood, Fidelity
    - (한국: 증권사 MTS/HTS)    
- 역할:
    - 주문 생성
    - 시장 데이터 조회
    - 사용자 UI 제공
### 2. 기관 고객

- 기관 고객은 **전문 거래 소프트웨어**로 **대량 주문**을 처리한다.
- 기관별 요구사항이 다름:
    - **연기금(pension funds)**
        - 목표: 안정적 수익
        - 특징: 거래 빈도는 낮지만 거래량은 큼
        - 필요 기능: **주문 분할(order splitting)** 등으로 **시장 영향(슬리피지/충격) 최소화**
            
    - **헤지펀드(hedge funds)**
        - 시장 조성(market making) 등으로 수익
        - **수수료 리베이트(commission rebates)** 도 수익원이 됨
        - 요구: **매우 낮은 응답시간(초저지연)**
        - 일반 사용자처럼 웹/모바일로 데이터 보면 안 됨(전용 피드/프로토콜 필요)
            
📌 **중요 포인트**
→ 거래소는 **일반(브로커) 트래픽**과 **기관(초저지연/고빈도) 트래픽**을 함께 감당해야 함.
## 3. 주문 유형

### 지정가 주문 (Limit Order)
- **가격을 고정**한 매수/매도 주문
- **즉시 체결이 안 될 수 있음**
- **부분 체결**(partial fill)도 가능
    
### 시장가 주문 (Market Order)
- **가격을 지정하지 않음**
- **시장가로 즉시 체결**
- 체결은 보장되지만, 상황에 따라 **불리한 가격**으로 체결될 수 있음
- 급변하는 시장에서 유용
    
📌 의미
→ 매칭 엔진은 **가격 우선 → 시간 우선** 규칙만 구현하면 됨

## 4. 시장 데이터 수준

미국 주식시장의 price quote를 **3단계 레벨**로 설명

### L1 (Level 1)
- **최고 매수 호가(best bid)**, **최저 매도 호가(ask)** 와 **각 수량(quantity)**
- 정의
    - 최고 매수: “사려는 사람이 지불할 의사가 있는 가장 높은 가격”
    - 최저 매도: “팔려는 사람이 받아들이는 가장 낮은 가격”
![[Pasted image 20260217041957.png]]
### L2 (Level 2)
- L1보다 더 많은 가격 레벨을 제공
- “깊이(depth)” = **호가창에서 체결 대기 물량을 어디까지 보여주는지**
![[Pasted image 20260217042126.png]]
### L3 (Level 3)
![[Pasted image 20260217042153.png]]
- L2보다 더 상세
- 각 가격 레벨에서 **개별 주문 단위(어떤 주문들이 대기 중인지)** 까지 보여줌

## 5. 봉 차트
![[Pasted image 20260217042212.png]]
- 봉 차트는 “특정 기간 동안의 주가 움직임”**을 한 개의 봉(캔들)로 요약해서 보여주는 방식.
    
- **한 봉(캔들) 1개가 담는 4가지 핵심 값**
    
    - **시작가(Open)**: 해당 기간 시작 가격
        
    - **종가(Close)**: 해당 기간 끝 가격
        
    - **최고가(High)**: 해당 기간 중 가장 높았던 가격
        
    - **최저가(Low)**: 해당 기간 중 가장 낮았던 가격
        
- 봉의 구성 요소
    
    - **실질적 주가 변화(Real body)**: 시작가~종가 구간
        
    - **위/아래 그림자(Upper/Lower shadow)**: 최고가·최저가까지의 꼬리
        
- 보통 지원되는 **시간 간격(캔들 단위)** 예시
    
    - **1분, 5분, 1시간, 1일, 1주, 1개월**
        
- 같은 데이터라도 **집계 주기에 따라 전혀 다른 봉**이 만들어짐
    
- **시장 데이터 흐름(Market Data Flow)**
    

👉 요약하면: _“캔들 하나 = 기간 내 OHLC(시가/종가/고가/저가) + 변동 범위”_

## 6. FIX (Financial Information eXchange Protocol)

- **FIX = 금융 정보 교환 프로토콜**
    
- **1991년에 만들어진**, 증권 거래 정보를 교환하기 위한 **기업 중립적(업계 표준) 통신 프로토콜**
    
- 역할/의미
    
    - 브로커·거래소·기관 시스템 간에 **주문/체결 같은 거래 메시지를 표준 포맷으로 주고받기** 위해 사용
        
- **중요 경로(Critical Path)** 에 위치
    
    - 주문
        
    - 체결 결과
        
- 특히:
    
    - 기관 고객
        
    - 초저지연 시스템
        
- 웹/REST/JSON ❌
    
- **FIX 또는 FIX 계열 바이너리 프로토콜** 선호
    

👉 요약하면: _“FIX는 거래 시스템끼리 주문/체결을 교환하는 업계 표준 메시지 규격(프로토콜)”_
![[Pasted image 20260217042229.png]]
**첨언**
- L1/L2/L3는 “정보량”과 “비용”이 커지는 구조라, 실제 서비스도 **유료 마켓데이터 티어링**(depth/latency/entitlement)으로 감
- FIX는 “표준”이지만, 초저지연 구간에서는 FIX를 그대로 쓰기보다 **바이너리 인코딩(SBE 등)** 으로 변환해 내부 파이프라인 효율을 끌어올리는 게 흔함
---

# 개략적 설계안

![[Pasted image 20260217042243.png]]

- **브로커**: 로빈후드/골드만삭스 등. 고객 주문을 받아 거래소로 전달하고, 시장데이터를 받아 고객에게 제공
    
- **클라이언트 게이트웨이(Client Gateway)**: 거래소 입구
    
    - 입력 유효성 검사, 속도 제한(rate limit), 인증, 정규화(normalization) 같은 **게이트키핑**
        
- **주문 관리자(Order Manager)**: 주문 접수 후 **리스크 체크 + 자금 확인 + 라우팅**
    
- **지갑(Wallet)**: 주문을 넣기 위한 **자금 충분 여부 확인**(및 이후 락/해제의 기반)
    
- **체결 엔진(Matching Engine)**: 주문 매칭의 핵심. 주문서/호가창 유지 + 체결 생성
    
- **호가창/주문서(Order Book)**: 종목별 매수/매도 주문 대기열
    
- **시퀀서(Sequencer)**: 주문/집행(체결) 기록을 **일정 순서로 정렬**해서
    
    재생(replay)해도 항상 동일 결과가 나오게(결정론) 보장
    
- **시장 데이터 게시 서비스(Market Data Publish)**: 체결/주문 스트림으로 **봉 차트·호가창** 등 시장데이터 생성/배포
    
- **데이터 서비스(Data Service)**: 브로커가 **실시간 시장데이터를 조회**하는 인터페이스
    
- **보고 서비스(Reporter) + DB**: 주문/집행 기록에서 리포팅용 필드를 모아 **DB에 적재**    

### A. 거래 흐름(Trading Flow) — **중요 경로(Critical Path)**, 지연 시간 가장 빡셈

**목표:** “주문이 들어와서 체결/통지까지”를 빠르게 끝내기

1. 고객이 브로커 앱에서 주문
    
2. 브로커가 거래소로 주문 전송
    
3. **클라이언트 게이트웨이** 통과(검증/제한/인증/정규화) → 주문 관리자로 전달
    
    4~5) **주문 관리자**가 규칙 기반 **위험성 점검(Risk check)** 수행
    
4. 위험성 점검 통과 시 **지갑 자금 충분 여부 확인**
    
    7~9) 주문을 **체결 엔진**으로 전달 → 매칭되면 **매수/매도 각각 1개씩 집행 기록(Execution/Fill) 2개 생성**
    
    - **시퀀서**가 주문/집행 기록을 정렬하여 **결정론적 결과** 보장
        
        10~14) 주문/집행 결과를 **클라이언트(브로커)로 통지**
        

> 핵심 포인트: 거래 흐름은 “빠름”이 최우선이라 **불필요한 후처리는 밖으로 뺀다**는 설계 의도가 깔려있어.

### B. 시장 데이터 흐름(Market Data Flow) — 읽기/분석용, **중요 경로 아님**

**목표:** “체결 결과를 시장데이터(봉/호가 등)로 가공해 브로커에 제공”

- **M1** 체결 엔진이 체결 시 **집행 기록 스트림** 생성 → 시장 데이터 게시 서비스로 전송
    
- **M2** 게시 서비스가 집행/주문 스트림으로 **봉 차트 + 호가창** 구성 → 데이터 서비스로 전달
    
- **M3** 시장 데이터를 **실시간 분석 전용 스토리지**에 저장 → 브로커가 데이터 서비스로 읽어 고객에게 제공
    

> 핵심 포인트: 시장데이터는 업데이트가 잦고(read 폭탄) 소비자가 많아서, 거래 핵심 경로와 **분리**하는 게 자연스럽다.

### C. 보고 흐름(Report Flow) — 적재/집계용, **중요 경로 아님**

**목표:** 규제/감사/정산/리포팅을 위한 “정형 레코드”를 DB에 남기기

- **R1~R2** Reporter가 주문/집행 기록에서 필요한 필드(예: client_id, price, quantity, order_type, filled_quantity, remaining_quantity 등)를 모아 **가공 레코드 생성 → DB 저장**
    

> 핵심 포인트: 보고는 “정확성/완전성”이 중요하지만, **ms 지연 요구는 거래 흐름만큼 높지 않다**.

## 핵심 메세지

- 거래소는 한 덩어리가 아니라,
    
    1. **거래(초저지연)**
        
    2. **시장데이터(대규모 팬아웃/읽기 폭탄)**
        
    3. **보고(적재/규제/감사)**
        
        이 3개를 **분리**해서 각각 다른 지연요건/확장전략으로 설계한다.
        
- 그리고 **결정론(deterministic) + 시퀀서**가 “고가용성”의 핵심 토대가 된다.
    

**첨언**
- 이 분리 덕분에 “거래 경로”는 **락/IO/DB/로깅**을 최대한 피하고, 시장데이터/보고는 **비동기 + eventual**로 확장하는 전형적인 구조가 됨.
    
- 실무에서는 여기서 “시장데이터는 브로커/벤더로 나가고”, “리포트는 규제기관/정산기관으로 나간다”로 외부 인터페이스가 더 늘어나며, **재처리/정합성 검증 파이프라인**이 필수로 붙음.
---
# 데이터 모델

## 1) 거래소의 3대 데이터 유형

1. **상품, 주문, 집행(Execution/Fill)**
    
    - “거래의 원본 기록(트랜잭션)”
        
2. **호가창/주문서(Order Book)**
    
    - “현재 시장 상태(대기 주문의 스냅샷/깊이)”
        
3. **봉 차트(Candles, OHLC)**
    
    - “체결 결과를 시간 단위로 집계한 시계열 요약”
        

## 2) 상품(Product)

- 의미: 거래 대상 주식(= 심벌)의 **정적 메타데이터**
    
- 예시 속성(책에서 언급)
    
    - 상품 유형
        
    - 거래 심벌 / UI 표시 심벌
        
    - 결산 통화(settle currency) / 호가 통화(quote currency)
        
    - **lot size(매매 수량 단위)**, **tick size(호가 가격 단위)**
        
    - description 등
        
- 특징
    
    - **자주 변경되지 않음**
        
    - 주로 **UI 표시용**
        
    - **아무 DB에나 저장 가능 + 캐시 적용 매우 적합**
        

## 3) 주문(Order) & 집행(Execution = Fill)

### 주문(Order)

- 의미: “매수/매도를 실행하라”는 명령
    
- 대표 필드(그림 13.12)
    
    - `orderID`, `productID`, `symbol`
        
    - `side`(buy/sell), `price`, `quantity`
        
    - `orderStatus`, `orderType`, `timeInForce`
        
    - `userID`, `accountID`, `clientOrderID`, `broker`
        
    - `entryTime`, `transactionTime`
        

### 집행(Execution / Fill)

- 의미: “체결 결과 레코드”
    
- 중요한 규칙
    
    - **모든 주문이 체결되진 않는다.**
        
    - 체결이 발생하면 매칭 엔진은 **한 번의 체결에 대해**
        
        - 매수 측을 나타내는 집행 1개 +
            
        - 매도 측을 나타내는 집행 1개
            
        - **총 2개의 집행 기록을 출력**한다.
            
- 대표 필드(그림 13.12)
    
    - `execID`, `orderID`, `symbol`
        
    - `side`, `price`, `quantity`
        
    - `orderStatus`, `orderType`, `execStatus`
        
    - `feeCurrency`, `feeRate`, `feeAmount`
        
    - `userID`, `accountID`, `transactionTime`
        

### 관계(그림 13.12의 포인트)

- **Product 1 ↔ Order 0..N**
    
- **Order 1 ↔ Execution 0..N**
    
- 이건 **DB 스키마가 아니라 논리 모델**(관계/개념을 설명하는 다이어그램)이라고 못 박음
    ![[Pasted image 20260217232123.png]]

### 중요 거래 경로

- 주문과 집행 기록을 DB 에 저장하지 않음
    
- 성능을 위해 **메모리에서 거래를 체결**하고, 공유 메모리를 활용해 주문과 집행 기록을 저장/공유 한다
    
- 주문/집행 기록은 빠른 복구를 위해 **시퀀서**에 저장하며, 데이터 저장은 장 마감 이후에 수행한다
    

## 4) 호가창/주문서(Order Book)

- 의미: “지금 이 순간 대기 중인 매수/매도 주문의 상태”
    
- API 형태로 감을 주는 모델(화면에 나온 L2 엔드포인트)
    
    - 응답이 `bids[]`, `asks[]` (가격-수량 배열)
        
    - L2는 **깊이(depth)** 를 파라미터로 받음
        
- 호가창은 **현재 상태(상태성 데이터)**, 주문/집행은 **이력(원장성 데이터)**

## 5) 봉 차트(Candles)

- 의미: 체결 가격 흐름을 **시간 구간(resolution) 단위로 집계한 OHLC**
    
- API 형태(화면에 나온 candles 엔드포인트)
    
    - `candles[]` 배열
        
    - 각 원소가 `open/close/high/low`
        
- 많은 종목 × 다양한 시간 간격(1분, 5분, 1시간 등)을 추적하면
    

👉 **메모리 사용량 폭증**

최적화 방법 2가지:

**① 링 버퍼 사용**

- 미리 메모리 할당
    
- 객체 재사용
    
- GC 비용 감소
    

**② 메모리 보관 개수 제한**

- 최근 데이터만 메모리 유지
    
- 나머지는 디스크 저장
    

**실제 저장 위치**

- 실시간 분석용:
    
    - **메모리 상주 칼럼형 DB (예: KDB)**
        
- 장 마감 후:
    
    - 이력 유지 전용 DB에 저장
        

- 봉 차트는 **집계 데이터(derived data)** 로, 원본인 집행(체결) 기록에서 만들어진다.

**첨언**

- “주문/체결은 DB에 안 쓴다”는 말은 **거래 핵심 경로에서 동기 DB write를 안 한다**는 뜻으로 이해해야함. 결국 규제/감사 때문에 영속화는 해야 하고, 보통은 **로그/저장소로 비동기 내리거나(append-only)** 장 후 처리로 밀어냄.
    
- 캔들은 derived data라서, 장애 시 “원본(체결 로그)”만 살아있으면 **재생성 가능**하도록 설계하는 게 정합성 측면에서 유리.
---
# 상세 설계

- **p99 지연시간**을 낮추고(=지연 분포 안정화),
    
- **중요 경로(Critical path)** 를 극단적으로 가볍게 만들어
    
- 단대단(end-to-end) 지연을 **수십 ms → 수십 μs** 수준으로 끌어내리는 설계 방향을 제시.
    

## 1) 성능

핵심 → 지연시간을 쪼개서 줄인다

### 지연시간 분해

- 지연시간 = **중요 경로 컴포넌트 실행시간의 합**
    
- 줄이는 방법은 2가지:
    
    1. **중요 경로에서 실행할 작업 수를 줄인다**
        
    2. **각 작업의 실행 시간을 줄인다**
        
        - 네트워크/디스크 사용량 줄이기
            
        - CPU 실행시간 자체 줄이기
            

### 중요 경로에 남는 최소 컴포넌트

- (책 기준) **게이트웨이 → 주문 관리자 → 시퀀서 → 체결 엔진**
    
- 메시지: _중요 경로에는 “꼭 필요한 것만”_
    
    → 심지어 **로깅도** 지연 때문에 중요 경로에서 빼기도 함.
    

## 2) 분산형(서버 여러 대) 설계의 한계

- 컴포넌트가 네트워크로 연결돼 있으면
    
    - 왕복 네트워크 지연만 해도 누적되며,
        
- 시퀀서가 디스크에 이벤트를 쓰면
    
    - 순차 쓰기 최적화해도 **디스크 지연은 수십 ms** 단위가 남음.
        
- 그래서 전체 end-to-end가 **수십 ms**로 가기 쉬움 → 초저지연 경쟁에선 부족.
    

## 3) 핵심 해법: 단일 서버 + 공유메모리(mmap) + 애플리케이션 루프

### 단일 서버 기반 설계(그림 13.15)

- 주문관리자 / 체결엔진 / 시장데이터 게시를 **같은 서버**에 배치해서
    
    - **네트워크 구간을 제거**
        
- 컴포넌트 간 통신은 네트워크 대신
    
    - **공유 메모리 기반 이벤트 저장소(mmap)** 로 연결
        

👉 메시지: _“네트워크/디스크를 줄이거나 없애면 μs로 내려간다.”_

## 4) 애플리케이션 루프(Application Loop) 설계
![[Pasted image 20260217232205.png]]
- 동작 방식: `while` 루프로 할 일을 계속 **polling** 하며 처리
    
- 목표:
    
    - 처리 시간을 **예측 가능**하게 만들어 p99를 낮추는 것
        
- 구현 전략:
    
    - CPU 효율을 위해 **단일 스레드**
        
    - 해당 스레드를 **특정 CPU 코어에 고정(pin)** (그림 13.16)
        
    - 입력(Netloop) 스레드는 별도, 핵심 처리는 루프 스레드가 담당
        

👉 메시지: _“멀티스레드로 throughput만 올리는 게 아니라, p99 안정화를 위해 실행 경로를 단순화/고정한다.”_

**첨언**

- 여기서 말하는 polling loop는 “CPU 낭비”처럼 보이지만, 초저지연 영역에서는 **컨텍스트 스위치/락/스케줄링 지터**를 줄이기 위해 흔한 선택
    
- 대신 비용은 명확함: **코어를 태운다 + 확장 방식이 바뀐다(수평 확장보다 파티셔닝/심벌 샤딩)**.
---
# 이벤트 소싱

- 전통 방식은 **DB에 “현재 상태(state)”만 저장**함
    
    → 문제가 생겼을 때 **왜 그 상태가 되었는지(원인 이벤트)** 를 추적하기가 어렵다.
    
- 이벤트 소싱은 반대로,
    
    - 상태를 직접 저장하기보다
        
    - 상태를 바꾸는 모든 이벤트를 **변경 불가능(immutable) 로그**로 남긴다.
        
    - 이 이벤트 로그를 **진실의 원천(source of truth)** 으로 삼는다.
        
- 그래서 **이벤트를 순서대로 재생(replay)** 하면 언제든 **주문 상태를 복구**할 수 있다.
    ![[Pasted image 20260217232222.png]]

## mmap 이벤트 저장소 = 초저지연 “메시지 버스”

- 이 책의 거래소는 “지연시간” 때문에 일반적인 Kafka 같은 걸 바로 쓰기 어렵고,
    
- 대신 **mmap(2)** 로 파일을 프로세스 메모리에 매핑해 **프로세스 간 고성능 공유 메모리 통신**을 만든다.
    
    - 특히 파일이 **/dev/shm(메모리 파일시스템)** 에 있으면 **디스크 I/O 없이** 공유 메모리로만 움직임.
        
- 결과적으로 중요 경로에서 **네트워크/디스크 접근 없이**, 이벤트 전달이 **마이크로초 미만**으로 가능해진다.
    

## 이벤트 소싱 설계 흐름
![[Pasted image 20260217232234.png]]
### 1) 외부(FIX) → 내부(SBE) 변환

- 외부 도메인은 **FIX** 로 거래 도메인과 통신.
    
- **게이트웨이**가 FIX를 **SBE(FIX over Simple Binary Encoding)** 로 변환해서 빠르게 처리.
    
- 그리고 이벤트 저장소 클라이언트를 통해, 표준 포맷의 **`NewOrderEvent`** 로 이벤트 저장소(mmap)에 기록.
    

### 2) 체결 엔진(내장 주문 관리자)이 이벤트로 상태를 만든다

- 체결 엔진에 **“주문 관리자(order manager)”가 내장된 라이브러리 형태**로 들어간다.
    
- 체결 엔진(내장 주문 관리자)은 이벤트 저장소에서 **`NewOrderEvent`를 pull(가져오기)** 해서
    
    - 유효성 검사
        
    - 내부 주문 상태에 반영
        
    - 해당 주문을 **담당 CPU 코어**로 전달
        

### 3) 체결 시 다시 이벤트로 발행

- 주문이 체결되면 **`OrderFilledEvent`** 를 생성해서 이벤트 저장소로 전송.
    
- **시장 데이터 프로세서 / 보고 서비스** 같은 다른 구성요소는 이벤트 저장소를 **구독(subscribe)** 하며,
    
    이벤트를 받을 때마다 각자 필요한 처리를 수행.
    

## 중앙 주문 관리자를 없애는 이유

- 중앙화된 주문 관리자에 질의/업데이트를 몰면 **네트워크 왕복/병목으로 지연이 늘 수 있음**.
    
- 이벤트 소싱의 특성상,
    
    - 각 컴포넌트가 동일 이벤트를 구독해 **자기 로컬에 주문 상태를 유지**해도
        
    - 결과는 **동일하고 재현 가능**하다.
        
- 그래서 주문 관리자는 “별도 서비스”가 아니라 **각 컴포넌트에 포함되는 재사용 라이브러리**로 둔다
    

## 시퀀서(Sequencer)는 어디로 갔나?

- 이벤트 소싱을 쓰면 **모든 메시지가 동일 이벤트 저장소로 들어가고**
    
- 이벤트 저장소 항목에 **`sequence` 필드**가 있는데,
    
    - 이 값은 **이벤트 저장소 내부의 시퀀서**가 찍는다.
        
- 중요한 제약:
    
    - **이벤트 저장소당 시퀀서는 1개**만 둔다
        
        (여러 개면 쓰기 권한 경쟁 → 락 경합 → 거래소에 치명적)
        

### mmap 환경 시퀀서 설계

- 각 컴포넌트는 자기 **링 버퍼**에 이벤트를 먼저 기록
    
- 시퀀서가 링 버퍼에서 이벤트를 **pull**
    
- 이벤트에 **순서 ID 부여** 후 이벤트 저장소에 기록
    
- 주 시퀀서 장애 대비 **백업 시퀀서**를 두면 가용성↑
    ![[Pasted image 20260217232249.png]]

**첨언**

- 여기 구조는 “단일 writer + 다중 reader” 철학이 강함. 초저지연에서 **write 경합이 곧 tail latency**로 직결되기 때문.
    
- 다만 이벤트 저장소 자체가 “심장”이라, 운영에서는 **백업/스냅샷/체크포인트 + 리플레이 시간 관리(RTO)**가 같이 따라와야 함.
    

---
# 고가용성

## 1) 목표: 99.99% 가용성

- 목표가 **99.99%**면, 거래소가 다운될 수 있는 시간은
    
    - **하루 8.64초 이하**
        
- 그래서 장애가 나면 **즉각 복구**가 전제.
    

## 2) 고가용성에서 먼저 할 일 2가지

1. **SPOF(단일 장애 지점)** 식별
    
    - 예: **체결 엔진 장애 = 거래소 재앙**
        
    - 그래서 주 인스턴스를 **다중화(복제)** 해야 함
        
2. **장애 감지 + 장애조치(Failover) 결정이 빨라야 함**
    
    - 짧은 시간 안에 “진짜 죽었나?”를 판단하고 전환해야 8.64초를 지킬 수 있음
        

## 3) Stateless vs Stateful 확장 전략이 다름

- **클라이언트 게이트웨이 같은 무상태**는
    
    - 서버 추가로 **수평 확장**이 쉽다.
        
- 반면 **주문 관리자/체결 엔진 같은 상태 저장 컴포넌트**는
    
    - “서버를 늘리는 것”만으로는 안 되고
        
    - **사본 간 상태 복제**가 가능해야 한다.
        

## 4) 주/부 체결 엔진(Primary/Secondary) 설계

핵심 아이디어: **둘 다 같은 이벤트를 처리하되, “쓰기 권한”은 하나만**.

- **주(Primary) 체결 엔진**
    
    - 이벤트 저장소로 **이벤트를 기록(write)** 하는 유일한 주체
        
    - `OrderFilledEvent` 같은 결과 이벤트를 저장소로 보냄
        
- **부(Secondary) 체결 엔진**
    
    - 이벤트 저장소에서 `NewOrderEvent`를 **수신하고 동일하게 처리**해서
        
    - **주와 동일한 상태를 유지**하지만
        
    - 이벤트 저장소로는 **기록하지 않음(write 금지)**
        
- **Failover**
    
    - 주가 다운되면 부가 즉시 **주 역할을 승계**하고
        
    - 그때부터 이벤트 저장소로 결과 이벤트를 기록하기 시작
    ![[Pasted image 20260217232303.png]]    

👉 포인트: 이벤트 소싱 + 결정론(deterministic) 덕분에

부는 “같은 입력 이벤트”를 처리하면 “같은 상태”가 되어 **즉시 승계**가 가능함.

## 5) 장애 감지 방법

- 일반적인 모니터링(하드웨어/프로세스) 외에
    
- 체결 엔진과 **박동(heartbeat) 메시지**를 주고받는 방식 제안
    
    - 정해진 시간 안에 heartbeat 못 받으면 장애로 판단
        

## 6) 이 설계의 한계와 확장 방향

### 한계

- 이 주/부 체결 엔진 설계는 **단일 서버 안에서만** 동작하는 형태로 설명됨.
    

### 더 높은 가용성을 위해 필요한 것

- 주/부 “체결 엔진”이 아니라
    
    - 주/부 “서버 클러스터”로 확장해야 함
        
- 주 서버의 **이벤트 저장소를 모든 부 서버로 복제**해야 함
    
- 복제 지연을 줄이기 위해
    
    - **reliable UDP 기반 브로드캐스트** 같은 접근을 언급
        
    - 예시로 **Aeron** 설계를 참고하라고 함
        

**첨언**

- Primary/Secondary에서 제일 무서운 건 **Split-Brain(둘 다 자신이 Primary라고 믿는 상황)**. 그래서 실무에서는 “쓰기 권한”을 보장하기 위해 **펜싱 토큰/리더 임대(lease)/쿼럼** 같은 장치가 붙는다

---
# 결함 내성

## 1) 결함 내성이 왜 필요한가

- 앞서의 **주/부(Primary/Secondary) 체결 엔진** 설계는 “대부분 잘 동작”하지만,
    
- **주 서버뿐 아니라 부 서버까지 전부 다운**되는 **희귀하지만 치명적** 상황이 존재함.
    
- 대형 기술 기업들은 이런 리스크를 줄이기 위해 **핵심 데이터를 여러 지역 데이터센터로 복제**한다.
    
    → 지진/대규모 정전 같은 **지역 단위 재해**까지 커버.
    

## 2) 결함 내성을 위해 반드시 답해야 하는 4가지 질문

1. **언제/어떻게 자동 전환(failover) 결정을 내릴까?**
    
2. **부 서버 중 새 리더(leader)는 어떻게 선출할까?**
    
3. **RTO(복구 시간 목표)**: 다운돼도 비즈니스가 버틸 수 있는 최대 시간은?
    
4. **RPO(복구 지점 목표)**: 어느 정도 **데이터 손실**을 허용할 수 있나?
    
    - 성능 저하 상태로라도 동작 가능한가?
        

## 3) “다운” 판정이 생각보다 어렵다 (오탐/연쇄 장애)

책이 강조하는 현실적인 함정 2가지:

- **오탐(false alarm)**
    
    - 모니터링이 잘못된 경보를 보내면
        
        → 불필요한 장애 극복 절차(자동 전환)가 발생
        
        → 시스템이 더 불안정해질 수 있음
        
- **연쇄 장애(cascading failure)**
    
    - 주 서버 다운 원인이 **코드 버그**라면
        
        → 부 서버로 넘겨도 같은 버그로 부 서버도 다운
        
        → 결국 **주/부 모두 중단** 가능
        

## 4) 현실적인 운영 전략(점진적 자동화)

- 새 시스템을 처음 런칭할 때는
    
    - **장애 복구를 수동으로** 수행하면서
        
    - **시그널/운영 경험**을 축적
        
- 충분히 자신이 생기면
    
    - 그때 **자동 감지 + 자동 복구**(failover)를 도입
        
- 이때 운영 경험을 빨리 쌓는 방법으로
    
    - **카오스 엔지니어링(Chaos Engineering)** 을 언급
        
        → 드물고 까다로운 장애 케이스를 일부러 드러내 학습/개선
        

## 5) 리더 선출(Leader Election): Raft로 푼다

장애 감지/전환 결정을 “올바르게” 할 수 있다고 가정하면, 다음은 **누가 리더(주)가 될지** 문제.

- 실무에서 검증된 리더 선출 알고리즘이 많고,
    
- 책은 예시로 **Raft**를 든다.
    

### Raft 핵심 아이디어

- 여러 서버가 각자 **mmap 이벤트 저장소**를 가지고 있고
    
- 현재 **리더(주)** 가 이벤트를 만들어서
    
    - 다른 노드(팔로어)들에게 **AppendEntries(RPC)** 로 복제
        
    - 팔로어는 받은 이벤트를 자기 mmap 저장소에 저장
    ![[Pasted image 20260217232321.png]]    

### 과반수 규칙(Quorum)

- 작업을 확정하려면 최소 투표수는 **n/2 + 1**
    
- 예: 5노드면 **3개**가 과반수(책에서 계산 예시 제시)
    

### Heartbeat와 선거(Election)

- 리더는 팔로어에게 **heartbeat(내용 없는 AppendEntries)** 를 주기적으로 보냄
    
- 일정 시간 heartbeat가 끊기면
    
    - 팔로어가 **선거 타이머** 시작
        
    - 가장 먼저 타임아웃된 팔로어가 **후보(candidate)** 가 되어
        
        - 다른 노드에 **RequestVote** 요청
            
    - 과반수 득표하면 새 리더가 됨
        

### Split vote(분할 투표)

- 여러 후보가 동시에 나오면 표가 갈려 리더가 안 정해질 수 있음
    
- 이 경우 타임아웃 후 **선거를 재시작**(term 개념으로 정리)
    

## 6) RTO / RPO

- **RTO(복구 시간 목표)**
    
    - 거래소는 _초 단위(second-level)_ RTO가 필요
        
    - 그래서 **자동 복구가 가능해야 함**
        
    - 이를 위해 서비스 우선순위 분류 + **성능 저하(degradation) 전략** 정의가 필요
        
- **RPO(복구 지점 목표)**
    
    - 거래소는 **데이터 손실을 용납할 수 없으므로 RPO ≈ 0**
        
    - Raft 같은 합의/복제 메커니즘을 쓰면
        
        - 데이터 사본을 여러 곳에 유지하고
            
        - 장애 시에도 새 리더가 **중단 없이** 이어서 처리 가능
            

**첨언**
- “RPO≈0”은 말은 쉬운데 구현이 빡셈: **리더가 확정(커밋)하기 전에는 외부에 확정 응답을 주면 안 되고**, 이게 곧 지연/처리량과 트레이드오프가 됨. 초저지연 거래소가 합의를 꺼리는 이유.
---
# 체결 알고리즘

## 1) 체결 알고리즘의 목적

- 들어온 **주문 이벤트(OrderEvent)** 를
    
    1. **순서 보장**(결정론)
        
    2. **검증**
        
    3. **신규/취소 분기**
        
    4. **매칭(match)** 수행
        
- 결과로
    
    - 주문서(OrderBook) 상태를 업데이트하고
        
    - 체결(Fill/Execution)을 생성한다.
        

## 2) 최상위 핸들러: `handleOrder(orderBook, orderEvent)`

### (1) 시퀀스 체크 = 결정론의 핵심

- 이벤트 소싱에서 이벤트 저장소가 부여한 `sequenceId`가 **정확히 1씩 증가**해야 함.
    
- 중간이 빠지거나 순서가 어긋나면
    
    - “지금 처리하면 상태가 달라질 수 있음” → 에러로 리턴.
        
- 이게 **리플레이(복구)해도 항상 같은 결과**를 만드는 장치.
    

### (2) 주문 기본 검증

- tick size / lot size / 가격 범위 등 “거래소 규칙” 위반을 컷.
    

### (3) 이벤트 → 내부 주문 객체 변환

- `NEW / CANCEL` 분기
    

## 3) 신규 주문 처리: `handleNew(orderBook, order)`

- 매수 주문은 **매도 호가창(sellBook)** 에서
    
- 매도 주문은 **매수 호가창(buyBook)** 에서
    
- 반대편과 매칭을 시도한다.
    

> 즉, “신규 주문이 들어오면 항상 반대편 책(book)과 먼저 맞춰본다(교차)”가 기본 규칙.

## 4) 취소 처리: `handleCancel(orderBook, order)`

핵심 규칙 1개:

- `orderMap`에 없으면:
    
    - 이미 체결되어 제거됐거나
        
    - 애초에 존재하지 않는 주문
        
- 그러면 취소 불가.
    

성공하면:

- 주문서를 제거(removeOrder)
    
- 상태를 CANCELED로 변경
    
- 성공 리턴
    

> 여기서 `orderMap`이 중요한 이유: **취소를 O(1)** 로 만들기 위한 인덱스.

## 5) 매칭 루프: `match(book, order)` (핵심)

### (1) 남은 수량 계산

- 부분 체결을 지원하므로 “남은 수량(leaves)” 중심으로 진행.
    

### (2) 해당 가격 레벨의 주문 리스트를 훑음

- `limitMap(price -> PriceLevel)` 로 **가격 레벨을 바로 찾고**
    
- 그 가격 레벨 안에서 `orders`(FIFO) 를 순서대로 매칭
    
    → **시간 우선(time priority)**
    

> 책 의사코드는 “지정가 주문 + 같은 가격 레벨 매칭” 중심으로 설명(개념 단순화)하고 있어.  
> 실전이라면 보통 “매수는 주문가격 이상인 매도 가격들(최저가부터)”처럼 **가격 레벨 범위 탐색**이 추가돼.

### (3) 매칭 결과

- counter 주문을 다 소진하면 제거
    
- 생성되는 체결 레코드(Fill/Execution)
    
    - 보통 “매수 1개 + 매도 1개 = 2개 fill” 발행(앞 파트와 연결)
        

## 6) 자료구조 포인트

- `orderMap`: `orderId -> Order` → 취소/조회 O(1)
    
- `limitMap`: `price -> PriceLevel(orders)` → 가격 레벨 접근 O(1)
    
- `PriceLevel.orders`: FIFO 리스트 → time priority 보장
    

**첨언**

- 실제 매칭은 보통
    - BUY: **최저 ask부터** 올라가며, ask ≤ buyPrice 범위에서 체결
    - SELL: **최고 bid부터** 내려가며, bid ≥ sellPrice 범위에서 체결  
        이때 자료구조는 hash map만으로 부족하고 보통 **정렬 구조(트리/힙/스킵리스트)** 또는 “최우선 가격 포인터”를 둠
---
# 결정론

## 결정론이란?

거래소 같은 초저지연/고정확 시스템에서 “결정론”은 크게 2가지로 나뉜다:

1. **기능적 결정론(Functional determinism)**
    
2. **지연 시간 결정론(Latency determinism)**
    

## 1) 기능적 결정론 (Functional determinism)

### 의미

- **같은 이벤트를 같은 순서로 재생(replay)하면 항상 같은 결과**가 나오는 성질
    

### 왜 중요?

- 장애 복구/재처리 시
    
    - “다시 돌렸더니 다른 체결이 났다”가 나오면 거래소는 망함.
        
- 그래서 **결과 재현성**이 필수.
    

### 책에서 연결하는 장치

- **시퀀서(Sequencer)** 또는 **이벤트 소싱(Event Sourcing)**
    
    - 모든 이벤트에 **연속된 sequenceId**를 붙여 처리 순서를 고정한다.
        

### 그림 포인트

- 실제 시간축에서는 이벤트 타임스탬프가 불규칙하지만,
    
- 시퀀서가 붙인 **연속된 점(순서)** 로 바꾸면
    
    - 기다릴 필요 없이 **빠르게 연속 재생** 가능 → 복구 시간 감소
    ![[Pasted image 20260217232344.png]]    

## 2) 지연 시간 결정론 (Latency determinism)

### 의미

- 각 거래의 처리 시간이 **거의 일정(예측 가능)** 하게 나오는 성질
    
- “평균”보다 **꼬리 지연(tail latency)** 가 낮아야 한다는 얘기
    

### 어떻게 측정하나?

- **p99(99퍼센타일)** 또는 **p99.99** 지연 시간으로 평가
    
- 책에서 언급한 측정 도구: **HdrHistogram**
    

### 지연 변동이 커질 때 흔한 원인(자바 예시)

- 자바에서는 **세이프포인트(safepoint)** 가 원인이 되는 경우가 많다.
    
- 대표 사례: HotSpot JVM의 **Stop-the-World GC**
    

## 한 줄 요약

- **기능적 결정론**: 같은 입력·같은 순서 ⇒ 같은 결과(복구/재현성)
    
- **지연 시간 결정론**: 거의 모든 거래가 비슷한 시간에 처리(p99 낮음)
    

**첨언**

- 결정론은 “고가용성의 기반”이기도 함: Secondary가 Primary와 같은 이벤트를 처리해 **동일 상태**를 유지하려면 기능적 결정론이 깨지면 안 됨.
    
- tail latency를 잡으려면 소프트웨어만이 아니라 **배포/런타임 선택(JVM vs native), 메모리 할당 전략, NUMA/CPU pinning**이 설계의 일부가 된다.
---
# 시장 데이터 게시 서비스 최적화

## 1) MDP(Market Data Publisher)가 하는 일

- 체결 엔진 출력 이벤트로
    
    1. 호가창(L2/L3) 재구축
        
    2. 봉 차트(OHLC) 생성
        
    3. 구독자에게 publish
        

## 2) 왜 최적화가 필요한가

- 수요가 크고(L2/L3, 기관 직접 수집),
    
- MDP는 동시에
    
    - 대량 업데이트 초고속 처리
        
    - 많은 구독자 팬아웃
        
    - 메모리 한계(종목×interval)  
        를 만족해야 함
        

## 3) 설계 요지

- 입력: 주문/체결 결과
    
- 내부:
    
    - 호가창 재구축
        
    - 봉 차트 생성(다양 interval)
        
    - 최근 데이터만 링 버퍼 보관, 나머지는 영속 저장소로
        
- 출력: 데이터 서비스 통해 조회
    
- 서비스 등급별 depth 제공(개인 L2 5레벨, 더 깊으면 유료)
   ![[Pasted image 20260217232404.png]] 

## 4) 핵심 최적화 포인트: 링 버퍼(Ring Buffer)

- 고정 크기 큐(원형)
    
- 장점:
    
    - 사전 할당, 객체 재사용 → GC 압박↓
        
    - lock-free 설계 가능
        
- 추가 최적화: Padding으로 false sharing 방지
    

## 5) ‘메모리 상한’ 관리(봉 차트)

- 최근 N개만 메모리 유지
    
- 오래된 데이터는 영속 저장소로
    


> MDP는 “가공 + 팬아웃” 파이프라인이고, p99 안정성과 메모리 한계를 위해 링 버퍼(사전할당·락프리·캐시라인 최적화)로 고속 처리 + 최근 데이터만 메모리에 유지한다.

**첨언**
- 시장데이터는 “쓰기보다 읽기/팬아웃”이 더 무서워서, 거래 경로와 분리하지 않으면 **읽기 폭탄이 거래를 죽인다**가 실전 교훈.
    
- depth 티어링은 기술 문제가 아니라 **비즈니스 모델(데이터 판매)**과 직결됨.
---
# 시장 데이터의 공정한 배포

## 1) 왜 “공정한 배포”가 중요한가

- 시장 데이터를 더 빨리 받는 것 = 구조적 이득
    
- 규제 거래소는 특정 구독자가 더 빠르게 받지 않게 **동시 전달**이 중요
    

## 2) 공정성이 깨지는 대표 사례

- 구독자 리스트 순서대로 항상 전송하면,
    
- “1번”이 항상 유리 → 모두가 1번 되려 몰림 → 공정성 붕괴
    

## 3) 완화 방법

### A. 멀티캐스트(multicast)

- 1회 전송으로 다수 수신자에 동시에 전달
    
- UDP 기반이라 손실 가능 → 누락/재전송 메커니즘 필요
    

### B. 무작위 순서 전송

- 매번 랜덤 순서로 보내 “항상 첫 번째 유리” 완화
    


> 시장 데이터는 “받는 순서 자체가 돈”이라, 규제 거래소는 멀티캐스트 같은 동시 전달로 공정성을 지키되, UDP 손실을 커버하는 복구 설계가 같이 필요하다.

**첨언**

- 공정성은 기술만이 아니라 규제/감사 관점에서도 중요해서, “왜 어떤 참여자가 더 빨랐는가”를 설명 못 하면 리스크가 커진다.
    
- 멀티캐스트+재전송은 보통 “빠른 채널(UDP multicast) + 느린 복구 채널(TCP/요청 기반)” 같은 **2채널 모델**로 푼다.


---