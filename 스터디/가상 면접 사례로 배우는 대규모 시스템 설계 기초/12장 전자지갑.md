# 개요

**전자 지갑 간 이체(wallet-to-wallet transfer)** 를 지원하는 백엔드를 설계하는 장.

결제 플랫폼이 제공하는 전자 지갑(예: 지갑에 돈을 넣어두고 결제/송금에 사용)에서, **같은 플랫폼 사용자끼리의 송금**

---
# 서비스 이해 및 설계 범위

### 1) 범위(Scope)

- **이체 기능에만 집중**
    - 다른 결제/충전/출금/정산 기능 등은 범위 밖

### 2) 처리량(Scale)

- 목표 TPS: **1,000,000 TPS**
    - “지갑 간 이체 요청” 기준

### 3) 정확성/일관성(Transactional guarantee)

- 전자 지갑은 정확성이 중요하니,
    - *DB 트랜잭션 보장만으로 충분한가?**를 질문
- 면접관: **충분하다(OK)**

### 4) “정확성 증명”과 재현성(Reproducibility)

- 단순히 트랜잭션만 맞다고 끝내지 않고,
    - **정확성을 사후 검증**해야 할 수 있음
- 대표 검증 방법: **조정(reconciliation)**
    - 내부 기록 vs 은행 명세서 비교
- 문제점:
    - 조정만으로는 “일관성이 깨졌다”는 건 알 수 있어도
    - **왜 깨졌는지 원인 추적이 어렵다**
- 그래서 목표:
    - **재현성(reproducibility)** 을 갖춘 시스템
    - 즉, **처음부터 데이터를 재생(replay)** 해서 언제든 **과거 잔액을 재구성**할 수 있게

### 5) 가용성(Availability)

- 목표: **99.99%**

### 6) 환전(FX)

- **환전 기능은 필요 없음**
---
# 개략적 추정

## 1. 전제 조건

- 목표 처리량: **1,000,000 TPS**
- 트랜잭션 기반 데이터베이스 사용
- 가정:
    - **일반적인 DB 노드 1대가 1,000 TPS 처리 가능**

## 2. 1차 계산

```
1,000,000 TPS ÷ 1,000 TPS/노드
= 1,000 노드
```

→ 100만 TPS를 처리하려면

👉 **1,000개의 DB 노드 필요**

## 3. 단순 계산의 문제

지갑 간 이체는 단일 연산이 아니다.

한 번의 이체에는:

1. 한 계좌에서 **출금**
2. 다른 계좌에 **입금**

즉,

```
1 이체 = 2 트랜잭션
```

## 4. 실제 필요한 TPS

```
1,000,000 이체 TPS
→ 2,000,000 DB TPS 필요
```

따라서:

```
2,000,000 ÷ 1,000 TPS/노드
= 2,000 노드
```

👉 **실제 필요한 노드 수: 2,000개**

---
# 개략적 설계

- 세 가지 개략적 설계안
    - 인메모리 기반
    - 데이터베이스 기반 분산 트랜잭션
    - 재현성을 갖춘 이벤트 소싱
---
# 인메모리 기반 설계

## 인메모리 샤딩이란?

- 전자지갑 시스템은 기본적으로 각 계정의 잔액을 **`<account, balance>`** 형태로 관리한다.
- 이 관계를 표현하기 좋은 구조는 **맵(Map) / 키-값 저장소(KV store)**.
- 인메모리 저장소로 흔한 선택이 **Redis**인데,
    - Redis 노드 1대로 **100만 TPS**는 어렵기 때문에
    - **Redis 클러스터**를 만들고 계정들을 여러 노드로 **균등 분산**해야 한다.
- 이 “여러 노드로 쪼개 분산하는 것”이 **파티셔닝(Partitioning) = 샤딩(Sharding)**.

## 샤딩 방식

- 키(계정 ID)의 해시 값을 이용해 파티션을 선택한다.

의사코드:

- `partition = hash(accountId) % n`

의미:

- n개의 Redis 노드(파티션)가 있으면
- 특정 accountId는 항상 같은 파티션으로 라우팅된다.

## 샤딩 메타데이터 관리

- “파티션 수 + 각 Redis 노드 주소”는 **어딘가 한 군데**에 저장해야 한다.
- 책은 이 저장소로 **ZooKeeper** 같은 “고가용 설정 저장소”를 추천한다.

즉,

- Redis cluster topology(파티션 정보)를
- **ZK가 소스 오브 트루스**로 들고 있는 형태.

## 지갑 서비스(Wallet Service)의 역할

이체 요청을 처리하는 애플리케이션 서버(무상태 서비스)로 “지갑 서비스”를 둔다.

지갑 서비스가 하는 일:

1. **이체 명령 수신**
2. 요청 **유효성 검증(validation)**
3. 유효하면,
    - `from_account`가 있는 Redis 노드를 찾고 **차감(-amount)**
        
    - `to_account`가 있는 Redis 노드를 찾고 **가산(+amount)**
        
        (두 계정이 서로 다른 Redis 노드에 있을 수 있음)
        

📌 중요한 특성

- 지갑 서비스는 **stateless**라서
    - 서버를 더 붙이는 방식으로 **수평 확장**이 쉽다.
## 작업 흐름

![[Pasted image 20260219142825.png]]
- 계정 A, B, C의 잔액이 3개 Redis 노드에 분산됨
- A→B로 $1 전송 요청이 오면
    - A가 속한 Redis엔 **1**
        
    - B가 속한 Redis엔 **+1**
        
        이렇게 **두 개의 업데이트**가 발생한다.
        

## 결론

- 이 설계는 “동작은” 하지만,
- **정확성 요구사항을 충족하지 못한다.**
    - 두 Redis 업데이트 중 하나만 성공하고 다른 하나가 실패하면 **불일치 발생**
        
    - 예: 첫 번째 차감 후, 두 번째 입금 전에 지갑 서비스가 죽으면
        
        → 돈이 “사라진” 상태가 될 수 있음
        

즉, 인메모리 샤딩은

- **확장성(throughput)은 확보**하지만
- **원자성(atomicity)·정확성(consistency)을 보장하지 못한다**
---
# 데이터베이스 기반 분산 트랜잭션 설계

## 분산 트랜잭션이란?

> 여러 노드에 걸친 작업을
> 
> **하나의 트랜잭션처럼 보이게 만드는 메커니즘**

## 구현 방법

### 1. 2PC

### 핵심 아이디어

- 한 노드만 결정하는 게 아니라
- **조정자(Coordinator)** 가 전체 트랜잭션을 관리

여기서는:

- 조정자 = 지갑 서비스

### 2단계 커밋(2PC) 흐름

### 🔹 1단계: Prepare 단계

1. 조정자가 각 DB에 작업 요청
2. 각 DB는:
    - 작업 실행
    - 락 설정
    - “준비 완료” 응답

```
DBA → ok
DB C → ok
```

### 🔹 2단계: Commit 단계

조정자가 응답을 모아서 판단:

- 모든 DB가 "YES" → 모두 커밋
- 하나라도 "NO" → 모두 롤백
![[Pasted image 20260219142910.png]]
### 장점

- 강한 원자성 보장
- ACID 보존
- 논리적으로 깔끔

### 단점

1. 블로킹 문제
    - 조정자가 죽으면?
    - 참여 노드는 **락 잡은 채 대기**
    - 시스템 정지 가능
2. 성능 문제
    - 네트워크 왕복 2번
    - 락 유지 시간 증가
    - TPS 감소
3. 확장성 문제
    - 100만 TPS 시스템에서
        
    - 모든 트랜잭션에 2PC?
        
        → 병목
## 2. TC/C

### TC/C란?

**TC/C = Try–Confirm/Cancel**

- 2PC와 달리 **보상 트랜잭션 기반의 고수준 분산 트랜잭션 패턴**
- 두 단계로 구성되지만,
- *각 단계가 서로 다른 “별도 트랜잭션”**이라는 점이 핵심 차이

### 2PC vs TC/C 차이

|구분|2PC|TC/C|
|---|---|---|
|단계 수|2단계|2단계|
|트랜잭션|하나의 분산 트랜잭션|단계별 독립 트랜잭션|
|락 유지|prepare 동안 장시간 락|각 단계는 즉시 커밋|
|블로킹|있음|없음|
|보상 로직|없음|cancel 단계에서 보상 수행|

👉 TC/C는 **락을 오래 잡지 않는 대신, 실패 시 되돌리는 보상 트랜잭션을 사용**

### TC/C 동작 흐름

전자지갑 예제:

A → C로 $1 이체

초기 상태:

```
A =$1C =$0
```

### 1단계: Try (시도)

조정자 = 지갑 서비스

### 작업

1. A 계정 DB에:
    
    ```
    UPDATEbalance= balance -1
    ```
    
2. C 계정 DB에는:
    
    - 아무것도 안 함 (NOP)
    - 항상 성공 응답

중요:

- 이 단계는 **이미 커밋된 트랜잭션**
- A 잔액은 이미 줄어듦
- 락은 짧게 걸렸다가 바로 해제

결과:

```
A =$0C =$0
```

### 2단계: Confirm (확정)

모든 DB가 OK를 반환했다면:

1. C 계정 DB에:
    
    ```
    UPDATEbalance= balance +1
    ```
    
2. A는 아무 작업 없음
    

최종 결과:

```
A =$0C =$1
```

### 실패 시: Cancel 단계

만약 Try 이후 Confirm에서 문제가 발생하면?

예:

- C 계정이 규제 계정
- C 업데이트 실패

그럼 Cancel 단계 실행:

1. A에 대해 보상 트랜잭션 수행:
    
    ```
    UPDATEbalance= balance +1
    ```
    
2. C는 NOP
    

최종 결과:

```
A =$1C =$0
```

→ 원상 복구

### 특징

1. 단계별 독립 트랜잭션
    - Try와 Confirm은 **별도 트랜잭션**
    - 락 오래 유지하지 않음
2. 보상 트랜잭션 기반
    - 이미 커밋된 작업을 “되돌리기” 위해
    - 반대 연산 수행
3. 블로킹이 없음
    - 2PC처럼 prepare 단계에서 락 잡고 대기하지 않음

### 문제

1. 비즈니스 로직에 **보상 로직이 필요**
2. 설계 복잡도 증가
3. 일시적으로는 불일치 상태가 존재 가능
    - Try 성공 후 Confirm 전까지

즉,

> TC/C는 강한 일관성보다는
> 
> 고성능 + eventual consistency에 가까운 방식

### 단계별 상태 테이블

TC/C는 **각 단계가 별도 트랜잭션**이라서, 실행 도중에 **조정자(지갑 서비스)** 가 죽거나 재시작하면 문제가 생김.

- “어디까지 했지?”
- “Try는 보냈나?”
- “Confirm/Cancel 중 뭐를 해야 하지?”
- “중복으로 실행하면 돈이 두 번 움직이지 않나?”

이 상태를 메모리에만 들고 있으면 재시작 시 **모든 진행 상황이 날아가서 복구 불가**가 되니까,

👉 **진행 상황을 DB에 영속화**해야 한다 = 단계별 상태 테이블.

### 단계별 상태 테이블에 저장해야 하는 정보

1. 분산 트랜잭션 식별/내용
    
    - **분산 트랜잭션 ID**
    - **트랜잭션 내용**(예: from, to, amount 등)
2. 각 DB 에 대한 Try 단계 상태
    
    각 참여 DB(예: A DB, C DB)마다 Try 요청이:
    
    - `not sent yet` (아직 안 보냄)
        
    - `has been sent` (보냈음)
        
    - `response received` (응답 받음)
        
        중 **어느 상태인지**
        
3. 두 번째 단계의 이름
    
    - `Confirm` 또는 `Cancel`
    - Try 단계 결과를 보고 **계산/결정**됨
4. 두 번째 단계의 상태
    
    - Confirm/Cancel 요청을 **보냈는지 / 응답 받았는지 / 완료됐는지** 등 진행 상태
5. 순서 어긋남 플래그
    
    - 예: Cancel이 Try보다 먼저 도착하는 경우 같은 **순서 꼬임**을 감지/처리하기 위한 표식
        
        (다음 절 “잘못된 순서로 실행된 경우”로 이어짐)
        

![[Pasted image 20260219142959.png]]
일반적으로 단계별 상태 테이블은 **돈을 인출하는 쪽** 이 위치한 데이터베이스에 둠
## 3. 사가

### 사가(Saga)란?

> 사가는 **여러 개의 로컬 트랜잭션을 순서대로 실행하고**,
> 
> 중간에 실패하면 **역순으로 보상 트랜잭션을 실행하는 방식**의
> 
> 애플리케이션 수준 분산 트랜잭션 패턴이다.

마이크로서비스 아키텍처에서 사실상 **표준적인 분산 트랜잭션 방식**.

### 기본 규칙

1. 모든 연산은 독립 로컬 트랜잭션
    - 각 서비스/DB는 자기 데이터베이스에 대해
        - **독립 트랜잭션으로 실행**
    - 글로벌 락 없음
2. 연산은 반드시 선형적 순서로 실행
    - 첫 번째 연산 완료 → 두 번째 시작
    - 두 번째 완료 → 세 번째 시작
    - 병렬 실행 ❌
3. 실패 시 보상 트랜잭션 실행
    - n개의 연산이 있다면
    - 최대 2n개의 연산 필요
        - n개 정상 연산
        - n개 보상(rollback) 연산

### 사가의 두 가지 방식

1. 분산 조율 (Choreography)
    
    - 중앙 조정자 없음
    - 각 서비스가
        - 다른 서비스의 이벤트를 구독
        - 이벤트 발생 시 다음 단계 실행
    - 완전 탈중앙
    
    📌 단점:
    
    - 서비스 수가 많으면
        - 상태 기계(state machine)가 복잡해짐
        - 디버깅 어려움
2. 중앙 집중형 (Orchestration)
    
    - 하나의 **조정자(coordinator)** 가 존재
    - 모든 단계를 순서대로 지시
    - 실패 시 보상 트랜잭션도 조정자가 실행
    
    📌 장점:
    
    - 복잡한 흐름 관리가 쉬움
    - 실무에서 더 선호되는 방식

## 📌 어떤 걸 선택해야 할까?

1. 지연 시간에 민감하지 않고, 서비스 수가 적다 ? → 사가
2. 지연 시간에 매우 민감하고, 서비스 수가 많다 → TC/C
---
# 이벤트 소싱 기반 설계

## 이벤트 소싱이 필요한 이유 ?

전자지갑 서비스는 외부 감사를 받을 수 있음.

감사자가 묻는 질문:

1. 특정 시점의 계정 잔액을 알 수 있나요?
2. 과거와 현재 잔액이 정확하다는 걸 어떻게 증명하나요?
3. 코드 변경 후에도 시스템 로직이 올바른지 어떻게 검증하나요?

👉 단순 “현재 잔액 컬럼”만 저장하는 구조로는

이 질문들에 체계적으로 답하기 어렵다.

## 이벤트 소싱 핵심 개념

- 명령 (Command)
- 이벤트 (Event)
- 상태 (State)
- 상태 기계 (State Machine)

### 1. 명령 (Command)

- 외부에서 들어오는 “의도(Intent)”
    
- 예:
    
    ```
    A에서 C로$1 송금하라
    ```
    

특징:

- 아직 사실(fact)이 아님
- 유효하지 않을 수 있음
- FIFO 큐에 저장됨
- 무작위성(randomness)이나 I/O를 포함할 수 있음

### 2. 이벤트 (Event)

명령이 검증되고 실행된 결과.

예:

```
A에서 C로$1 송금을 완료하였다.
```

특징:

- 하나의 명령 → 0개 이상 이벤트 생성 가능
- 이벤트 생성 과정에는 I/O나 랜덤성이 개입될 수 있음
- 이벤트 순서는 명령 순서를 따라야 함 (FIFO)

### 명령 vs 이벤트 차이

|항목|명령|이벤트|
|---|---|---|
|성격|의도|사실(Fact)|
|시제|미래|과거|
|검증 여부|미검증|검증 완료|
|결정론|비결정적 가능|결정론적|

### 3. 상태 (State)

상태는 이벤트가 적용되면서 변화한다.

전자지갑에서 상태는:

```
Map<AccountId,Balance>
```

예:

```
A: 10C: 5
```

이벤트가 적용되면 상태가 변한다.

### 4. 상태 기계 (State Machine)

상태 기계는:

“명령을 검증하고 이벤트를 생성”하고, “이벤트를 적용하여 상태를 갱신”하는 로직 집합

```
명령(Command) → 유효성 검사 → 이벤트(Event) 생성
```

이벤트 적용 → 상태 갱신

```
이벤트(Event) + 현재 상태(State)
→ 새로운 상태(New State)
```

모든 이벤트를 순서대로 재생하면 항상 동일한 상태가 만들어진다.

👉 이것이 “결정론”의 핵심
## 이벤트 소싱의 핵심 철학

```jsx
모든 이벤트를 저장
상태는 이벤트를 재생해서 계산
```

이벤트 로그가 **진실의 원천(Source of Truth)**

![[Pasted image 20260219143105.png]]
## 장점

1. 완전한 감사 추적 (Audit Trail)
2. 특정 시점 상태 복원 가능
3. 버그 검증 가능
4. 분산 트랜잭션 문제 완화

## CQRS

명령(Command)과 질의(Query)의 책임을 분리하는 설계 철학

**쓰기 경로(write path)** 와 **읽기 경로(read path)** 를 완전히 분리하는 아키텍처 패턴.

### CQRS 가 필요한 이유 ?

이벤트 소싱 구조에서는:

- 상태는 이벤트를 통해서만 변경됨
- 외부 클라이언트가 잔액을 직접 조회할 방법이 없음

상태를 공개하지 않고 **이벤트를 공개**하여, **외부에서 이벤트를 이용해 상태를 재구성** 한다.

### CQRS 핵심

1. 쓰기 경로
    - 명령 처리
    - 이벤트 생성
    - 상태 변경
    - 단 하나의 “쓰기 상태 기계”만 존재
2. 읽기 경로
    - 읽기 전용 상태 기계는 여러 개 존재 가능
    - 다양한 형태의 뷰(View)를 생성

예:

- 현재 잔액 뷰
- 기간별 거래 내역 뷰
- 감사용 상태 복원 뷰
- 분석용 집계 뷰

### 특징

1. 읽기/쓰기 모델이 다를 수 있음
    - 쓰기는 이벤트 로그 기반
    - 읽기는 RDB, 캐시, 별도 DB 가능
2. 읽기 전용 상태 는 실제 상태보다 늦을 수 있음
    - Eventual Consistency (결과적 일관성)
3. 상태는 이벤트에서 재구성 됨
    - 이벤트가 진실의 원천(Source of Truth)

![[Pasted image 20260219143125.png]]
### 장점

1. 읽기와 쓰기 확장 독립적
2. 읽기 모델 최적화 가능
3. 감사/복원/분석에 유리
4. 복잡한 도메인 모델에서 유연함

### 단점

1. 설계 복잡도 증가
2. 이벤트 정합성 관리 필요
3. 결과적 일관성 모델 이해 필요
---
# 상세 설계

높은 성능과 안정성 및 확장성을 달성하기 위한 기법을 살펴본다

- 고성능 이벤트 소싱
- 분산 이벤트 소싱
---
# 고성능 이벤트 소싱

기본 이벤트 소싱(예: **Kafka=명령/이벤트 저장소 + DB=상태 저장**)은

- **한 번에 하나의 이벤트 처리**
- **원격 저장소(Kafka/DB) 네트워크 왕복**
- 외부 시스템 통신 등으로 인해 **지연/처리량 한계**가 생김.
- 3가지 최적화 방법을 살펴본다.

## 1. 파일 기반 명령/이벤트 목록

- Kafka 같은 **원격 큐** 대신
- **로컬 디스크 파일**에 명령/이벤트를 저장
- “최근 명령/이벤트”는 **메모리에 캐시**해서 디스크에서 재로드하는 비용을 줄임
- 네트워크 전송 시간 제거
- 이벤트 목록은 **append-only(추가만 가능)** 구조
- append는 **순차 쓰기(sequential write)** 이고 OS가 매우 최적화
    - HDD에서도 잘 동작
    - 경우에 따라 “무작위 메모리 접근”보다 빠를 수도 있다

### mmap 활용한 구현

![[Pasted image 20260219143413.png]]
- `mmap`은 **디스크 파일을 메모리 배열처럼 매핑**하는 기술
- OS가 파일 일부를 메모리에 캐시해
    - 읽기/쓰기 속도를 높인다
- append-only 파일이라면
    - 필요한 데이터가 거의 항상 메모리에 있어
    - **실질적으로 메모리처럼 빠르게** 동작할 수 있음
- “디스크에 쓰는 동시에 최신 데이터는 자동으로 메모리에 캐시”되는 구조
## 2. 파일 기반 상태 저장

기존엔 잔액(상태)을 **네트워크로 붙는 RDB**에 저장했는데, 네트워크를 제거하자는 방향.

선택지:

- 로컬 파일 기반 RDB: **SQLite**
- 로컬 파일 기반 KV: **RocksDB**

책의 선택:

- **RocksDB**
    - 내부 자료구조가 **LSM 트리(쓰기 최적화)**
    - 최근 데이터 캐시로 읽기 성능도 보완
- “상태 DB까지 로컬로” 가져와 네트워크 홉이 사라짐.
![[Pasted image 20260219143438.png]]

원격 Kafka/RDB → **로컬 파일(mmap) + 로컬 RocksDB** 로 치환해서 네트워크 지연을 줄이고 순차 I/O + OS 캐시를 최대 활용한다.

## 3. 스냅숏

이벤트 소싱의 재현(replay)은 기본적으로 “처음 이벤트부터 끝까지” 다시 읽어야 해서 느릴 수 있음.

주기적으로 상태 기계를 멈추고 **현재 상태를 파일로 저장** = 스냅숏

스냅숏은 **불변(immutable)** 이고 특정 시점의 상태를 의미

- 복구/조회 시 스냅숏을 로드한 뒤, 스냅숏 생성 시점 이후 이벤트만 재생하면 됨

일반적으로 HDFS 와 같은 객체 저장소에 저장한다.
![[Pasted image 20260219143448.png]]

---
# 신뢰할 수 있는 고성능 이벤트 소싱

## 신뢰성 관점 분석

- 노드가 하는 일은 **데이터 + 연산**인데,
- **데이터만 살아있으면 연산(계산)은 다른 노드에서 다시 돌려 복구 가능**
- 반대로 **데이터가 손실되면 결과를 복원할 방법이 없음**
- 시스템 신뢰성 문제는 대부분 **데이터 신뢰성 문제**라고 본다.

### 현재 설계에서 다루는 4가지 데이터

- **파일 기반 명령(command)**
- **파일 기반 이벤트(event)**
- **파일 기반 상태(state)**
- **상태 스냅숏(snapshot)**

### 무엇을 강하게 보장해야 하는가 ?

- 상태(state)와 스냅숏(snapshot)
    - 둘 다 **이벤트 목록을 재생(replay)** 하면 언제든 다시 만들 수 있음. **이벤트 목록의 신뢰성** 을 보장하는 것이 핵심.
- 명령(command)
    - **이벤트 생성은 결정론적이지 않을 수 있음**
    - 난수 / 외부 I/O 같은 무작위 요소가 개입 가능 **명령 신뢰성만으로는 이벤트 재현성 보장 불가**
- 이벤트(event)
    - 이벤트는 상태(잔액)를 바꾼 **과거의 사실(historical fact)**
    - **불변(immutable)** 이고 상태 재구성의 근거가 됨
    - “높은 신뢰성을 강하게 보장해야 할 유일한 데이터는 **이벤트**”

## 합의 기반 복제 (Consensus-based replication)

이벤트 목록을 여러 노드에 복제하되, 다음을 보장해야 함.

1. **데이터 손실 없음**
2. **로그 내 이벤트의 상대적 순서가 모든 노드에서 동일**

이 목표를 만족하는 대표 접근이 **합의(Consensus)** 이고, 책은 예시로 **Raft**를 든다.

### Raft 핵심

- 과반수(quorum)가 살아있으면 시스템은 동작
    - 3노드 → 1노드 장애 허용
    - 5노드 → 2노드 장애 허용
- 노드 역할 3가지
    1. **리더(leader)**: 외부 명령을 받고 이벤트로 변환, 복제를 주도
    2. **팔로어(follower)**: 리더를 따라 로그 복제
    3. **후보(candidate)**: 리더 선출 과정에서 등장
- 결과적으로 **append-only 이벤트 로그**가 모든 노드에서 동일하게 유지됨
![[Pasted image 20260219143523.png]]
## 고 신뢰성 아키텍처

- 여러 **이벤트 소싱 노드**가 Raft로 하나의 “로그 합의 그룹”을 형성
- **리더 노드**
    - 명령 수신 → 이벤트로 변환 → 로컬 이벤트 로그에 append
    - Raft가 팔로어들에게 복제
- **팔로어 포함 모든 노드**
    - 동일한 이벤트 로그를 처리(replay)해서
    - 로컬 상태(state)를 업데이트
- 읽기/질의는 노드들에서 제공 가능(설계에 따라)

![[Pasted image 20260219143545.png]]

<aside> 💡

고성능을 위해 “로컬 파일(mmap) + RocksDB”로 이벤트 소싱을 구성하되, **신뢰성은 결국 ‘이벤트 로그’에 달려 있으므로**, 이벤트 로그를 **Raft 같은 합의 기반 복제로 여러 노드에 동일 순서로 복제**해 SPOF를 제거하는 설계

</aside>

---
# 분산 이벤트 소싱

- 로컬 파일 + mmap + RocksDB
- Raft 기반 복제

로 **신뢰성(SPOF 제거)** 은 해결했지만, 하지만 아직 두 가지 문제가 남아 있음

1. CQRS 의 응답 지연 문제
    - 클라이언트는 “지갑 잔액이 언제 업데이트됐는지” 즉시 알기 어려움
    - 주기적 폴링(polling)에 의존
    - 실시간 응답 보장이 어렵다
2. 단일 Raft 그룹의 한계
    - 하나의 Raft 그룹은 결국 하나의 로그
    - 처리 용량이 물리적으로 제한됨
    - 100만 TPS 이상이면
        - **샤딩(sharding) 필요**
        - **분산 트랜잭션 필요**

## 풀 vs 푸시 모델

### 풀 모델

![[Pasted image 20260219143628.png]]

클라이언트가 주기적으로 상태를 질의한다

단점:

- 실시간 아님
- 너무 자주 질의하면 과부하

### 개선된 풀 모델 (Reverse Proxy)

![[Pasted image 20260219143640.png]]

```jsx
클라이언트
→ Reverse Proxy
→ 이벤트 소싱 노드
```

클라이언트 대신 프락시가 상태를 대신 질의해, 클라이언트 로직이 단순화 되었지만 여전히 실시간은 아님.

## 단일 Raft 그룹 확장 한계

Raft 그룹은:

- 하나의 로그
- 하나의 리더
- 직렬 append

→ CPU / I/O 한계 존재

100만 TPS라면, 하나의 Raft 그룹으로는 부족함

## 분산 이벤트 소싱

하나의 Raft 그룹이 아닌, 여러 개의 Raft 그룹으로 나눈다.

### 파티셔닝 기준

전자지갑에서는 보통:

```
accountId 기준 샤딩
```

예:

```
hash(accountId) % N
```

각 파티션은:

- 독립된 Raft 그룹
- 독립된 이벤트 로그
- 독립된 상태 저장소

### 장점

1. 수평 확장 가능
2. 각 그룹이 독립적으로 처리
3. TPS 선형 확장

### 문제점

다음 케이스:

```
A → C 송금
```

A와 C가 서로 다른 파티션이라면? → **분산 트랜잭션 문제 발생**

그래서:

- 샤딩
- 분산 트랜잭션
- 이벤트 소싱
- CQRS

가 결합된 구조가 필요해진다.

### 분산 이벤트 소싱의 특징

단일 노드 이벤트 소싱:

```
명령 → 이벤트 → 상태
```

분산 이벤트 소싱:

```
명령
→ 파티션 선택
→ 해당 Raft 그룹에 append
→ 각 그룹에서 상태 업데이트
```

> 교차 파티션 트랜잭션은 TC/C 또는 Saga로 처리한다.

<aside> 💡

분산 이벤트 소싱은 단일 Raft 로그의 한계를 극복하기 위해 계정 기준으로 여러 Raft 그룹으로 샤딩하고, 각 그룹이 독립적으로 이벤트를 처리하도록 확장한 구조다. 다만 **교차 파티션 트랜잭션**을 위한 별도 분산 트랜잭션 설계가 필요하다.

</aside>